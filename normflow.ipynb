{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a869702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import drjit as dr\n",
    "import drjit.nn as nn\n",
    "from drjit.opt import Adam, GradScaler\n",
    "from drjit.llvm.ad import (\n",
    "    Texture2f,\n",
    "    TensorXf,\n",
    "    TensorXf16,\n",
    "    Float16,\n",
    "    Float32,\n",
    "    ArrayXf16,\n",
    "    Array2f,\n",
    "    Array3f,\n",
    ")\n",
    "import mitsuba as mi\n",
    "\n",
    "mi.set_variant(\"llvm_ad_rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96435d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = dr.rng(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref = TensorXf(\n",
    "    iio.imread(\n",
    "        \"https://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2024/06/wave-128.png\"\n",
    "    )\n",
    "    / 256\n",
    ")\n",
    "ref = dr.mean(ref, axis=-1)[:, :, None]\n",
    "ref = ref / dr.mean(ref, axis=None)\n",
    "tex = Texture2f(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce211b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "ref_np = ref.numpy()[:, :, 0]\n",
    "dist_ref = mi.DiscreteDistribution2D(ref_np)\n",
    "\n",
    "x, _, _ = dist_ref.sample(rng.random(mi.Point2f, (2, 1_000_000)))\n",
    "x = mi.Point2f(x) / mi.Vector2f(ref_np.shape[0], ref_np.shape[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "hist, _, _ = np.histogram2d(x.y, x.x, bins=ref_np.shape[0], density=True)\n",
    "ax[0].set_title(\"sampled\")\n",
    "ax[0].imshow(hist)\n",
    "ax[0].set_title(\"evaluated\")\n",
    "ax[1].imshow(ref_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f4ace",
   "metadata": {},
   "source": [
    "Normalizing flows can be used to both sample from a learned distribution, but\n",
    "also evaluate the probability density function for a given sample. This makes\n",
    "them very useful in computer graphics, where both properties are often\n",
    "required.\n",
    "\n",
    "A normalizing flow is represented by an invertible function $f_\\theta$. To\n",
    "sample random variables $X$ from the learned distribution, we sample latent\n",
    "variables $Z$ from a normal gaussian distribution $Z \\sim p_Z = N(0, 1)$, and\n",
    "apply the inverse flow $X = f^{-1}_\\theta(Z)$.\n",
    "\n",
    "We parameterize the normalizing flows with coupling and permutation layers\n",
    "$f_{i;\\theta}$, such that $X = f_{0;\\theta} \\circ f_{1;\\theta} \\circ \\dots\n",
    "f_{D;\\theta} (Z)$. To train the network, we maximize the log sum of the\n",
    "estimated probability of sampling the sample i.e. $max \\sum \\text{log}\n",
    "p_{X;\\theta}(X_i)$. To compute this probability, we can sum over the log\n",
    "determinant of the layers, $p_{X;\\theta}(X) = \\text{log} \\left\\vert \\text{det} {\\partial z\n",
    "\\over \\partial x} \\right\\vert_{\\theta} + \\text{log} p_{Z}(Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b06188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def square_to_std_normal(x: dr.ArrayBase, y: dr.ArrayBase):\n",
    "    r = dr.sqrt(-2.0 * dr.log(1.0 - x))\n",
    "    phi = 2.0 * dr.pi * y\n",
    "\n",
    "    s, c = dr.sincos(phi)\n",
    "    return c * r, s * r\n",
    "\n",
    "\n",
    "def std_normal_pdf(z: dr.ArrayBase):\n",
    "    return dr.inv_two_pi * dr.exp(-0.5 * dr.square(z))\n",
    "\n",
    "\n",
    "def log_std_normal_pdf(z: dr.ArrayBase):\n",
    "    return dr.log(std_normal_pdf(z))\n",
    "    return dr.log(dr.inv_two_pi) + (-0.5 * dr.square(z))\n",
    "\n",
    "\n",
    "class FlowLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec: ...\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]: ...\n",
    "\n",
    "\n",
    "class PermutationLayer(FlowLayer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        z = list(z)\n",
    "        z.reverse()\n",
    "        x = nn.CoopVec(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        x = list(x)\n",
    "        x.reverse()\n",
    "        z = nn.CoopVec(x)\n",
    "        ldj = Float16(0)\n",
    "        return z, ldj\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        result = PermutationLayer()\n",
    "\n",
    "        return result, size\n",
    "\n",
    "\n",
    "class CouplingLayer(FlowLayer):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"net\": nn.Sequential,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, n_layers: int = 3, width: int = 1, n_activations: int = 64\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        sequential = []\n",
    "        sequential.append(nn.Linear(width, n_activations))\n",
    "        for i in range(n_layers - 2):\n",
    "            sequential.append(nn.Linear(n_activations, n_activations))\n",
    "            sequential.append(nn.ReLU())\n",
    "        sequential.append(nn.Linear(n_activations, width * 2))\n",
    "\n",
    "        self.net = nn.Sequential(*sequential)\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        This function represents the inverse evaluation of the coupling layer,\n",
    "        i.e. $X = f^{-1}_\\theta(Z)$.\n",
    "        \"\"\"\n",
    "        z: list = list(z)\n",
    "        d = len(z) // 2\n",
    "        id, z2 = z[:d], z[d:]\n",
    "        p = ArrayXf16(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d, :], p[d:, :]\n",
    "        x2 = (z2 - mu) * dr.exp(-a)\n",
    "        x = nn.CoopVec(id, x2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        r\"\"\"\n",
    "        This function evaluates the foward flow $Z = f_\\theta(X)$, as well as\n",
    "        the log jacobian determinant.\n",
    "        \"\"\"\n",
    "        x = list(x)\n",
    "        d = len(x) // 2\n",
    "        id, x2 = x[:d], x[d:]\n",
    "        p = ArrayXf16(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d, :], p[d:, :]\n",
    "        z2 = x2 * dr.exp(a) + mu\n",
    "        z = nn.CoopVec(id, z2)\n",
    "        ldj = dr.sum(a)\n",
    "        return z, ldj\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        net, _ = self.net._alloc(dtype, size // 2, rng)\n",
    "\n",
    "        result = CouplingLayer()\n",
    "        result.net = net\n",
    "\n",
    "        return result, size\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"layers\": list[FlowLayer],\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args: FlowLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = args\n",
    "\n",
    "    def sample_base_dist(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        sample = list(sample)\n",
    "        z = []\n",
    "        for i in range(0, len(sample), 2):\n",
    "            x, y = square_to_std_normal(Float32(sample[i]), Float32(sample[i + 1]))\n",
    "            z.append(Float16(x))\n",
    "            z.append(Float16(y))\n",
    "\n",
    "        return nn.CoopVec(*z)\n",
    "\n",
    "    def eval_base_dist_log(self, z: nn.CoopVec) -> dr.ArrayBase:\n",
    "        return dr.sum([log_std_normal_pdf(z) for z in z])\n",
    "\n",
    "    def log_p(self, x: nn.CoopVec) -> Float16:\n",
    "        \"\"\"\n",
    "        This function calculates the log probability of sampling a given value\n",
    "        `x`.\n",
    "        \"\"\"\n",
    "\n",
    "        log_p = dr.zeros(x.type)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, ldj = layer.forward(x)\n",
    "            log_p += ldj\n",
    "\n",
    "        z = x\n",
    "\n",
    "        log_p += self.eval_base_dist_log(z)\n",
    "        return log_p\n",
    "\n",
    "    def sample(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        Sample a function from the learned target distribution $X \\sim\n",
    "        p_{X;\\theta}$, given a sample from the uniform distribution.\n",
    "        \"\"\"\n",
    "        z = self.sample_base_dist(sample)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            z = layer.inverse(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        layers = []\n",
    "        for l in self.layers:\n",
    "            l_new, size = l._alloc(dtype, size, rng)\n",
    "            layers.append(l_new)\n",
    "\n",
    "        result = Flow(*layers)\n",
    "        return result, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    CouplingLayer(),\n",
    "    # PermutationLayer(),\n",
    "    # CouplingLayer(),\n",
    "    # PermutationLayer(),\n",
    "    # CouplingLayer(),\n",
    "]\n",
    "flow = Flow(*layers)\n",
    "\n",
    "flow: Flow = flow.alloc(TensorXf16, rng=rng)\n",
    "\n",
    "weights, flow = nn.pack(flow, \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr=0.001, params={\"weights\": Float32(weights)})\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "batch_size = 2**14\n",
    "n = 10\n",
    "\n",
    "iterator = tqdm.tqdm(range(n))\n",
    "for it in iterator:\n",
    "    weights[:] = Float16(opt[\"weights\"])\n",
    "\n",
    "    x, _, _ = dist_ref.sample(rng.random(mi.Point2f, (2, batch_size)))\n",
    "    x = mi.Point2f(x) / mi.Vector2f(ref_np.shape[0], ref_np.shape[1])\n",
    "    x = nn.CoopVec(ArrayXf16(x))\n",
    "\n",
    "    log_p = flow.log_p(x)\n",
    "    loss_kl = -dr.mean(log_p)\n",
    "\n",
    "    dr.backward(scaler.scale(loss_kl))\n",
    "    scaler.step(opt)\n",
    "\n",
    "    if (it + 1) % 1 == 0:\n",
    "        iterator.set_postfix({\"loss_kl\": loss_kl.numpy().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = ArrayXf16(flow.sample(nn.CoopVec(rng.random(ArrayXf16, (2, 1_000_000)))))\n",
    "hist, _, _ = np.histogram2d(\n",
    "    x[1], x[0], bins=ref_np.shape[0], density=True, range=[[0, 1], [0, 1]]\n",
    ")\n",
    "\n",
    "x = rng.random(ArrayXf16, (2, 1_000_000))\n",
    "log_p = flow.log_p(nn.CoopVec(x))\n",
    "p = dr.exp(log_p)\n",
    "hist2, _, _ = np.histogram2d(\n",
    "    x[1],\n",
    "    x[0],\n",
    "    bins=ref_np.shape[0],\n",
    "    density=True,\n",
    "    weights=p,\n",
    "    range=[[0, 1], [0, 1]],\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].set_title(\"sampled\")\n",
    "ax[0].imshow(hist)\n",
    "ax[0].set_title(\"evaluated\")\n",
    "ax[1].imshow(hist2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
