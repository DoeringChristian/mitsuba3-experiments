{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76838cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "import drjit.nn as nn\n",
    "import imageio.v3 as iio\n",
    "\n",
    "from drjit.opt import Adam, GradScaler\n",
    "from drjit.auto.ad import (\n",
    "    Texture2f,\n",
    "    TensorXf,\n",
    "    TensorXf16,\n",
    "    Float16,\n",
    "    Float32,\n",
    "    Array2f,\n",
    "    Array3f,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a1b81",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "ref = TensorXf(\n",
    "    iio.imread(\n",
    "        \"https://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2024/06/wave-128.png\"\n",
    "    )\n",
    "    / 256\n",
    ")\n",
    "tex = Texture2f(dr.mean(ref, axis=-1)[:, :, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6a9f0",
   "metadata": {},
   "source": [
    "Normalizing flows can be used to both sample from a learned distribution, but\n",
    "also evaluate the probability density function for a given sample. This makes\n",
    "them very useful in computer graphics, where both properties are often\n",
    "required.\n",
    "\n",
    "A normalizing flow is represented by an invertible function $f_\\theta$. To\n",
    "sample random variables $X$ from the learned distribution, we sample latent\n",
    "variables $Z$ from a normal gaussian distribution $Z \\sim p_Z = N(0, 1)$, and\n",
    "apply the inverse flow $X = f^{-1}_\\theta(Z)$.\n",
    "\n",
    "We parameterize the normalizing flows with coupling and permutation layers\n",
    "$f_{i;\\theta}$, such that $X = f_{0;\\theta} \\circ f_{1;\\theta} \\circ \\dots\n",
    "f_{D;\\theta} (Z)$. To train the network, we maximize the log sum of the\n",
    "estimated probability of sampling the sample i.e. $max \\sum \\text{log}\n",
    "p_{X;\\theta}(X_i)$. To compute this probability, we can sum over the log\n",
    "determinant of the layers, $p_{X;\\theta}(X) = \\text{log} \\left\\vert \\text{det} {\\partial z\n",
    "\\over \\partial x} \\right\\vert_{\\theta} + \\text{log} p_{Z}(Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PermutationLayer(nn.Module): ...\n",
    "\n",
    "\n",
    "class CouplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers: int = 3, n_activations: int = 64) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        sequential = []\n",
    "        for i in range(n_layers - 1):\n",
    "            sequential.append(nn.Linear(n_activations, n_activations))\n",
    "            sequential.append(nn.ReLU())\n",
    "\n",
    "        sequential.append(nn.Linear(n_activations, n_activations))\n",
    "\n",
    "        self.net = nn.Sequential(*sequential)\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        This function represents the inverse evaluation of the coupling layer,\n",
    "        i.e. $X = f^{-1}_\\theta(Z)$.\n",
    "        \"\"\"\n",
    "        z: list = list(z)\n",
    "        d = len(z) // 2\n",
    "\n",
    "        id, z2 = z[:d, d:]\n",
    "\n",
    "        p = list(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d], p[d:]\n",
    "\n",
    "        x2 = (z2 - mu) * dr.exp(-a)\n",
    "\n",
    "        x = nn.CoopVec(id, x2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        r\"\"\"\n",
    "        This function evaluates the foward flow $Z = f_\\theta(X)$, as well as\n",
    "        the log jacobian determinant.\n",
    "        \"\"\"\n",
    "\n",
    "        x = list(x)\n",
    "        d = len(x) // 2\n",
    "\n",
    "        id, x2 = x[:d], x[d:]\n",
    "\n",
    "        p = list(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d], p[d:]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
