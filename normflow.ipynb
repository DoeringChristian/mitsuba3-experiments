{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82742110",
   "metadata": {},
   "source": [
    "# Normalizing Flows\n",
    "In this tutorial, we will implement normalizing flows using the Dr.Jit\n",
    "compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d7f2a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb28faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import imageio.v3 as iio\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import drjit as dr\n",
    "import drjit.nn as nn\n",
    "from drjit.opt import Adam, GradScaler\n",
    "from drjit.auto.ad import (\n",
    "    Texture2f,\n",
    "    TensorXf,\n",
    "    TensorXf16,\n",
    "    Float16,\n",
    "    Float32,\n",
    "    ArrayXf16,\n",
    "    Array2f,\n",
    "    Array3f,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792c2ef",
   "metadata": {},
   "source": [
    "First we initialize a random number generator, which will be used as the\n",
    "source of randomness in this tutorial, ensuring that variables remain\n",
    "uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcdf03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = dr.rng(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7634d2",
   "metadata": {},
   "source": [
    "We use a standard normal distribution for the base distribution of the flow\n",
    "model. Since our random number generator produces uniformly distributed\n",
    "values, we define this function to warp them into gaussian distributed\n",
    "values. The flow model also requires us to evaluate the logarithm of this\n",
    "distribution, and we therefore also define this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab44901a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def square_to_std_normal(sample: Array2f):\n",
    "    \"\"\"\n",
    "    This function takes a sample from a uniform square distribution, and\n",
    "    transforms it into a sample from a standard normal distribution.\n",
    "    \"\"\"\n",
    "    r = dr.sqrt(-2.0 * dr.log(1.0 - sample[0]))\n",
    "    phi = 2.0 * dr.pi * sample[1]\n",
    "\n",
    "    s, c = dr.sincos(phi)\n",
    "    return Array2f(c * r, s * r)\n",
    "\n",
    "\n",
    "def log_std_normal_pdf(z: dr.ArrayBase):\n",
    "    return dr.log(dr.inv_two_pi) - 0.5 * dr.square(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cc5c8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SpiralDistr:\n",
    "    def __init__(self) -> None: ...\n",
    "    def sample(\n",
    "        self,\n",
    "        rng: dr.random.Generator,\n",
    "        n: int,\n",
    "    ):\n",
    "        sample1 = rng.random(Float32, n)\n",
    "        sample2 = rng.random(Array2f, (2, n))\n",
    "\n",
    "        sample1 = sample1 * 2 - 1\n",
    "        t = dr.sqrt(dr.abs(sample1))\n",
    "        r = t * 4 * dr.sign(sample1)\n",
    "        phi = t * dr.two_pi * 1.5\n",
    "\n",
    "        s, c = dr.sincos(phi)\n",
    "        x = Array2f(c * r, s * r)\n",
    "        y = square_to_std_normal(sample2) * 0.15\n",
    "\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59113f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageDistr:\n",
    "    def __init__(\n",
    "        self,\n",
    "        uri: str = \"data/albert.jpg\",\n",
    "    ) -> None:\n",
    "        import mitsuba as mi\n",
    "\n",
    "        mi.set_variant(\"cuda_ad_rgb\")\n",
    "\n",
    "        self.mi = mi\n",
    "\n",
    "        img = iio.imread(uri)\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.mean(-1)\n",
    "        img = img / img.sum(None)\n",
    "        self.shape = img.shape\n",
    "\n",
    "        distr = mi.DiscreteDistribution2D(img)\n",
    "        self.distr = distr\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        rng: dr.random.Generator,\n",
    "        n: int,\n",
    "    ):\n",
    "        mi = self.mi\n",
    "        x, _, _ = self.distr.sample(rng.random(mi.Point2f, (2, n)))\n",
    "        m = max(self.shape[0], self.shape[1])\n",
    "        x = mi.Point2f(x) / mi.Vector2f(m)\n",
    "        x = mi.Point2f(x.x, 1.0 - x.y)\n",
    "        return x * 8 - 4\n",
    "\n",
    "\n",
    "# ref = SpiralDistr()\n",
    "ref = ImageDistr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cec97a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDistr' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m      2\u001b[0m hist_range \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m]]\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[30;43mref\u001b[39;49m\u001b[30;43m.\u001b[39;49m\u001b[30;43msample\u001b[39;49m(rng, \u001b[38;5;241m100_000\u001b[39m)\n\u001b[1;32m      5\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mhist2d(x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m], bins\u001b[38;5;241m=\u001b[39mn_bins, \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39mhist_range)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageDistr' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "\n",
    "n_bins = 256\n",
    "hist_range = [[-5, 5], [-5, 5]]\n",
    "\n",
    "x = ref.sample(rng, 1_000_000)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.hist2d(x[0], x[1], bins=n_bins, range=hist_range)\n",
    "ax.set_title(\"sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9bfa1",
   "metadata": {},
   "source": [
    "Normalizing flows can be used to both sample from a learned distribution, but\n",
    "also evaluate the probability density function for a given sample. This makes\n",
    "them very useful in computer graphics, where both properties are often\n",
    "required.\n",
    "\n",
    "A normalizing flow is represented by an invertible function $f_\\theta$. To\n",
    "sample random variables $X$ from the learned distribution, we sample latent\n",
    "variables $Z$ from a normal gaussian distribution $Z \\sim p_Z = N(0, 1)$, and\n",
    "apply the inverse flow $X = f^{-1}_\\theta(Z)$.\n",
    "\n",
    "We parameterize the normalizing flows with coupling and permutation layers\n",
    "$f_{i;\\theta}$, such that $X = f_{0;\\theta} \\circ f_{1;\\theta} \\circ \\dots\n",
    "f_{D;\\theta} (Z)$. To train the network, we maximize the log sum of the\n",
    "estimated probability of sampling the sample i.e. $max \\sum \\text{log}\n",
    "p_{X;\\theta}(X_i)$. To compute this probability, we can sum over the log\n",
    "determinant of the layers, $p_{X;\\theta}(X) = \\text{log} \\left\\vert \\text{det} {\\partial z\n",
    "\\over \\partial x} \\right\\vert_{\\theta} + \\text{log} p_{Z}(Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d65613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    r\"\"\" \"\"\"\n",
    "\n",
    "    DRJIT_STRUCT = {}\n",
    "\n",
    "    def __call__(self, arg: nn.CoopVec, /) -> nn.CoopVec:\n",
    "        return (\n",
    "            0.5\n",
    "            * arg\n",
    "            * (1 + dr.tanh(dr.sqrt(2 / dr.pi) * (arg + 0.044715 * arg * arg * arg)))\n",
    "        )\n",
    "\n",
    "\n",
    "x = dr.linspace(Float32, -5, 5, 1000)\n",
    "y = GELU()(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b812f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwoAlign(nn.Module):\n",
    "    r\"\"\" \"\"\"\n",
    "\n",
    "    DRJIT_STRUCT = {}\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "        return self, size if size % 2 == 0 else size + 1\n",
    "\n",
    "    def __call__(self, arg: nn.CoopVec, /) -> nn.CoopVec:\n",
    "        tp = arg.type\n",
    "        arg = list(arg)\n",
    "        if len(arg) % 2 != 0:\n",
    "            arg.append(tp(0))\n",
    "        return nn.CoopVec(*arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95658440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FlowLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec: ...\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]: ...\n",
    "\n",
    "\n",
    "class PermutationLayer(FlowLayer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        z = list(z)\n",
    "        z.reverse()\n",
    "        x = nn.CoopVec(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        x = list(x)\n",
    "        x.reverse()\n",
    "        z = nn.CoopVec(x)\n",
    "        ldj = Float16(0)\n",
    "        return z, ldj\n",
    "\n",
    "\n",
    "class CouplingLayer(FlowLayer):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"net\": nn.Sequential,\n",
    "        \"config\": tuple,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, n_hidden: int = 1, width: int = 2, n_activations: int = 32\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = (width,)\n",
    "\n",
    "        sequential = []\n",
    "        # sequential.append(nn.TriEncode(16, 0))\n",
    "        sequential.append(TwoAlign())\n",
    "        sequential.append(nn.Linear(-1, n_activations))\n",
    "        sequential.append(nn.ReLU())\n",
    "        for i in range(n_hidden):\n",
    "            sequential.append(nn.Linear(n_activations, n_activations))\n",
    "            sequential.append(nn.ReLU())\n",
    "        sequential.append(nn.Linear(n_activations, width))\n",
    "\n",
    "        self.net = nn.Sequential(*sequential)\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        This function represents the inverse evaluation of the coupling layer,\n",
    "        i.e. $X = f^{-1}_\\theta(Z)$.\n",
    "        \"\"\"\n",
    "        z: list = ArrayXf16(z)\n",
    "        d = len(z) // 2\n",
    "        id, z2 = z[:d, :], z[d:, :]\n",
    "        p = ArrayXf16(self.net(nn.CoopVec(id)))\n",
    "        log_s, b = p[:d, :], p[d:, :]\n",
    "        x2 = (z2 - b) * dr.exp(-log_s)\n",
    "        x = nn.CoopVec(id, x2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        r\"\"\"\n",
    "        This function evaluates the foward flow $Z = f_\\theta(X)$, as well as\n",
    "        the log jacobian determinant.\n",
    "        \"\"\"\n",
    "        x = ArrayXf16(x)\n",
    "        d = len(x) // 2\n",
    "        id, x2 = x[:d, :], x[d:, :]\n",
    "        p = ArrayXf16(self.net(nn.CoopVec(id)))\n",
    "        log_s, b = p[:d, :], p[d:, :]\n",
    "        z2 = x2 * dr.exp(log_s) + b\n",
    "        z = nn.CoopVec(id, z2)\n",
    "        ldj = dr.sum(log_s)\n",
    "        return z, ldj\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        (width,) = self.config\n",
    "        if width < 0:\n",
    "            width = size\n",
    "\n",
    "        net, _ = self.net._alloc(dtype, width // 2, rng)\n",
    "\n",
    "        result = CouplingLayer()\n",
    "        result.net = net\n",
    "\n",
    "        return result, size\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"layers\": list[FlowLayer],\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args: FlowLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = args\n",
    "\n",
    "    def sample_base_dist(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        sample = list(sample)\n",
    "        z = []\n",
    "        for i in range(0, len(sample), 2):\n",
    "            x, y = square_to_std_normal(Array2f(sample[i], sample[i + 1]))\n",
    "            z.append(Float16(x))\n",
    "            z.append(Float16(y))\n",
    "\n",
    "        return nn.CoopVec(*z)\n",
    "\n",
    "    def eval_log_base_dist(self, z: nn.CoopVec) -> dr.ArrayBase:\n",
    "        return dr.sum([log_std_normal_pdf(z) for z in z])\n",
    "\n",
    "    def log_p(self, x: nn.CoopVec) -> Float16:\n",
    "        \"\"\"\n",
    "        This function calculates the log probability of sampling a given value\n",
    "        `x`.\n",
    "        \"\"\"\n",
    "\n",
    "        log_p = dr.zeros(x.type)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, ldj = layer.forward(x)\n",
    "            log_p += Float32(ldj)\n",
    "\n",
    "        log_p += self.eval_log_base_dist(x)\n",
    "        return log_p\n",
    "\n",
    "    def sample(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        Sample a function from the learned target distribution $X \\sim\n",
    "        p_{X;\\theta}$, given a sample from the uniform distribution.\n",
    "        \"\"\"\n",
    "        z = self.sample_base_dist(sample)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            z = layer.inverse(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        layers = []\n",
    "        for l in self.layers:\n",
    "            l_new, size = l._alloc(dtype, size, rng)\n",
    "            layers.append(l_new)\n",
    "\n",
    "        result = Flow(*layers)\n",
    "        return result, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    *[CouplingLayer(), PermutationLayer()] * 4,\n",
    "    # CouplingLayer(),\n",
    "    # PermutationLayer(),\n",
    "    # CouplingLayer(),\n",
    "]\n",
    "flow: Flow = Flow(*layers)\n",
    "\n",
    "flow = flow.alloc(TensorXf16, rng=rng)\n",
    "\n",
    "weights, flow = nn.pack(flow, \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ArrayXf16(flow.sample(nn.CoopVec(rng.random(ArrayXf16, (2, 1_000_000)))))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.hist2d(x[0], x[1], bins=n_bins, range=hist_range)\n",
    "ax.set_title(\"flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eedfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dr.linspace(Float16, -4, 4, 1_000)\n",
    "n = list(flow.layers[0].net(nn.CoopVec(x)))\n",
    "plt.plot(x, n[0], label=\"log_s\")\n",
    "plt.plot(x, n[1], label=\"b\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ca745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr=0.001, params={\"weights\": Float32(weights)})\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "batch_size = 2**14\n",
    "n = 10_000\n",
    "its = []\n",
    "losses = []\n",
    "\n",
    "iterator = tqdm.tqdm(range(n))\n",
    "for it in iterator:\n",
    "    weights[:] = Float16(opt[\"weights\"])\n",
    "\n",
    "    x = ref.sample(rng, n)\n",
    "    x = nn.CoopVec(ArrayXf16(x))\n",
    "\n",
    "    log_p = flow.log_p(x)\n",
    "    log_p[dr.isnan(log_p)] = 0\n",
    "    log_p[dr.isinf(log_p)] = 0\n",
    "    loss_kl = -dr.mean(log_p)\n",
    "\n",
    "    dr.backward(scaler.scale(loss_kl))\n",
    "    scaler.step(opt)\n",
    "\n",
    "    if (it + 1) % 10 == 0:\n",
    "        loss = loss_kl.numpy().item()\n",
    "        losses.append(loss)\n",
    "        its.append(it)\n",
    "        iterator.set_postfix({\"loss_kl\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(its, losses)\n",
    "plt.ylabel(\"KL loss\")\n",
    "plt.xlabel(\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8334779",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ArrayXf16(flow.sample(nn.CoopVec(rng.random(ArrayXf16, (2, 1_000_000)))))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.hist2d(x[0], x[1], bins=n_bins, range=hist_range)\n",
    "ax.set_title(\"flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dr.linspace(Float16, -4, 4, 1_000)\n",
    "n = list(flow.layers[0].net(nn.CoopVec(x)))\n",
    "plt.plot(x, n[0], label=\"log_s\")\n",
    "plt.plot(x, n[1], label=\"b\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b942dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dr.linspace(Float16, -4, 4, 1_000)\n",
    "n = list(flow.layers[2].net(nn.CoopVec(x)))\n",
    "plt.plot(x, n[0], label=\"log_s\")\n",
    "plt.plot(x, n[1], label=\"b\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
