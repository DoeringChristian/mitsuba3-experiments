{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "import drjit.nn as nn\n",
    "import imageio.v3 as iio\n",
    "\n",
    "from drjit.opt import Adam, GradScaler\n",
    "from drjit.auto.ad import (\n",
    "    Texture2f,\n",
    "    TensorXf,\n",
    "    TensorXf16,\n",
    "    Float16,\n",
    "    Float32,\n",
    "    Array2f,\n",
    "    Array3f,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f93d15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "ref = TensorXf(\n",
    "    iio.imread(\n",
    "        \"https://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2024/06/wave-128.png\"\n",
    "    )\n",
    "    / 256\n",
    ")\n",
    "tex = Texture2f(dr.mean(ref, axis=-1)[:, :, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c9755",
   "metadata": {},
   "source": [
    "Normalizing flows can be used to both sample from a learned distribution, but\n",
    "also evaluate the probability density function for a given sample. This makes\n",
    "them very useful in computer graphics, where both properties are often\n",
    "required.\n",
    "\n",
    "A normalizing flow is represented by an invertible function $f_\\theta$. To\n",
    "sample random variables $X$ from the learned distribution, we sample latent\n",
    "variables $Z$ from a normal gaussian distribution $Z \\sim p_Z = N(0, 1)$, and\n",
    "apply the inverse flow $X = f^{-1}_\\theta(Z)$.\n",
    "\n",
    "We parameterize the normalizing flows with coupling and permutation layers\n",
    "$f_{i;\\theta}$, such that $X = f_{0;\\theta} \\circ f_{1;\\theta} \\circ \\dots\n",
    "f_{D;\\theta} (Z)$. To train the network, we maximize the log sum of the\n",
    "estimated probability of sampling the sample i.e. $max \\sum \\text{log}\n",
    "p_{X;\\theta}(X_i)$. To compute this probability, we can sum over the log\n",
    "determinant of the layers, $p_{X;\\theta}(X) = \\text{log} \\left\\vert \\text{det} {\\partial z\n",
    "\\over \\partial x} \\right\\vert_{\\theta} + \\text{log} p_{Z}(Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ba146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def uniform_to_std_normal(x: dr.ArrayBase):\n",
    "    y = dr.zeros_like(x)\n",
    "    r = dr.sqrt(-2.0 * dr.log(1.0 - x))\n",
    "    phi = 2.0 * dr.pi * y\n",
    "\n",
    "    c = dr.cos(phi)\n",
    "    return c * r\n",
    "\n",
    "\n",
    "def uniform_to_std_normal_pdf(z: dr.ArrayBase):\n",
    "    return dr.inv_two_pi * dr.exp(-0.5 * dr.square(z))\n",
    "\n",
    "\n",
    "class FlowLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec: ...\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]: ...\n",
    "\n",
    "\n",
    "class PermutationLayer(FlowLayer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        z = list(z)\n",
    "        z.reverse()\n",
    "        x = nn.CoopVec(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        x = list(x)\n",
    "        x.reverse()\n",
    "        z = nn.CoopVec(x)\n",
    "        ldj = Float16(0)\n",
    "        return z, ldj\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        result = PermutationLayer()\n",
    "\n",
    "        return result, size\n",
    "\n",
    "\n",
    "class CouplingLayer(FlowLayer):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"net\": nn.Sequential,\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_layers: int = 3, n_activations: int = 64) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        sequential = []\n",
    "        for i in range(n_layers - 1):\n",
    "            sequential.append(nn.Linear(n_activations, n_activations))\n",
    "            sequential.append(nn.ReLU())\n",
    "\n",
    "        sequential.append(nn.Linear(n_activations, n_activations))\n",
    "\n",
    "        self.net = nn.Sequential(*sequential)\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        This function represents the inverse evaluation of the coupling layer,\n",
    "        i.e. $X = f^{-1}_\\theta(Z)$.\n",
    "        \"\"\"\n",
    "        z: list = list(z)\n",
    "        d = len(z) // 2\n",
    "\n",
    "        id, z2 = z[:d, d:]\n",
    "\n",
    "        p = list(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d], p[d:]\n",
    "\n",
    "        x2 = (z2 - mu) * dr.exp(-a)\n",
    "\n",
    "        x = nn.CoopVec(id, x2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        r\"\"\"\n",
    "        This function evaluates the foward flow $Z = f_\\theta(X)$, as well as\n",
    "        the log jacobian determinant.\n",
    "        \"\"\"\n",
    "\n",
    "        x = list(x)\n",
    "        d = len(x) // 2\n",
    "\n",
    "        id, x2 = x[:d], x[d:]\n",
    "\n",
    "        p = list(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d], p[d:]\n",
    "\n",
    "        z2 = x2 * dr.exp(a) + mu\n",
    "        z = nn.CoopVec(id, z2)\n",
    "        ldj = dr.sum(a)\n",
    "\n",
    "        return z, ldj\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        net, n_activations = self.net._alloc(dtype, size, rng)\n",
    "\n",
    "        result = CouplingLayer(n_activations=n_activations)\n",
    "        result.net = net\n",
    "\n",
    "        return result, size\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"layers\": list[FlowLayer],\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args: FlowLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = args\n",
    "\n",
    "    def sample_base_dist(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        return nn.CoopVec(*[uniform_to_std_normal(x) for x in sample])\n",
    "\n",
    "    def eval_base_dist(self, z: nn.CoopVec) -> dr.ArrayBase:\n",
    "        return dr.prod([uniform_to_std_normal_pdf(z) for z in z])\n",
    "\n",
    "    def log_p(self, x: nn.CoopVec) -> Float16:\n",
    "        \"\"\"\n",
    "        This function calculates the log probability of sampling a given value\n",
    "        `x`.\n",
    "        \"\"\"\n",
    "\n",
    "        log_p = dr.zeros(x.dtype)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, ldj = layer.forward(x)\n",
    "            log_p += ldj\n",
    "\n",
    "        z = x\n",
    "\n",
    "        log_p += dr.log(self.eval_base_dist(z))\n",
    "        return log_p\n",
    "\n",
    "    def sample(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        Sample a function from the learned target distribution $X \\sim\n",
    "        p_{X;\\theta}$, given a sample from the uniform distribution.\n",
    "        \"\"\"\n",
    "        z = self.sample_base_dist(sample)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            z = layer.inverse(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        layers = []\n",
    "        for l in self.layers:\n",
    "            l_new, size = l._alloc(dtype, size, rng)\n",
    "            layers.append(l_new)\n",
    "\n",
    "        result = Flow(*layers)\n",
    "        return result, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    CouplingLayer(),\n",
    "    PermutationLayer(),\n",
    "    CouplingLayer(),\n",
    "    PermutationLayer(),\n",
    "    CouplingLayer(),\n",
    "]\n",
    "flow = Flow(*layers)\n",
    "\n",
    "flow = flow.alloc(TensorXf16)\n",
    "\n",
    "weights, flow = nn.pack(flow, \"training\")\n",
    "print(weights.shape)\n",
    "print(flow)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
