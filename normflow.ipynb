{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import drjit as dr\n",
    "import drjit.nn as nn\n",
    "from drjit.opt import Adam, GradScaler\n",
    "from drjit.auto.ad import (\n",
    "    Texture2f,\n",
    "    TensorXf,\n",
    "    TensorXf16,\n",
    "    Float16,\n",
    "    Float32,\n",
    "    ArrayXf16,\n",
    "    Array2f,\n",
    "    Array3f,\n",
    ")\n",
    "import mitsuba as mi\n",
    "\n",
    "mi.set_variant(\"cuda_ad_rgb\", \"llvm_ad_rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = dr.rng(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b2f18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def square_to_std_normal(sample: Array2f):\n",
    "    r = dr.sqrt(-2.0 * dr.log(1.0 - sample[0]))\n",
    "    phi = 2.0 * dr.pi * sample[1]\n",
    "\n",
    "    s, c = dr.sincos(phi)\n",
    "    return Array2f(c * r, s * r)\n",
    "\n",
    "\n",
    "def log_std_normal_pdf(z: dr.ArrayBase):\n",
    "    return dr.log(dr.inv_two_pi) - 0.5 * dr.square(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SpiralDistr:\n",
    "    def __init__(self) -> None: ...\n",
    "    def sample(self, sample1: Float32, sample2: Array2f):\n",
    "        sample1 = dr.sqrt(sample1)\n",
    "        r = sample1 * 4\n",
    "        phi = sample1 * dr.two_pi * 2\n",
    "\n",
    "        s, c = dr.sincos(phi)\n",
    "        x = Array2f(s * r, c * r)\n",
    "        y = square_to_std_normal(sample2) * 0.2\n",
    "\n",
    "        return x + y\n",
    "\n",
    "\n",
    "ref = SpiralDistr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_bins = 256\n",
    "hist_range = [[-5, 5], [-5, 5]]\n",
    "\n",
    "sample1 = rng.random(Float32, 100_000)\n",
    "sample2 = rng.random(Array2f, (2, 100_000))\n",
    "x = ref.sample(sample1, sample2)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.hist2d(x[0], x[1], bins=n_bins, range=hist_range)\n",
    "ax.set_title(\"sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b093f5f",
   "metadata": {},
   "source": [
    "Normalizing flows can be used to both sample from a learned distribution, but\n",
    "also evaluate the probability density function for a given sample. This makes\n",
    "them very useful in computer graphics, where both properties are often\n",
    "required.\n",
    "\n",
    "A normalizing flow is represented by an invertible function $f_\\theta$. To\n",
    "sample random variables $X$ from the learned distribution, we sample latent\n",
    "variables $Z$ from a normal gaussian distribution $Z \\sim p_Z = N(0, 1)$, and\n",
    "apply the inverse flow $X = f^{-1}_\\theta(Z)$.\n",
    "\n",
    "We parameterize the normalizing flows with coupling and permutation layers\n",
    "$f_{i;\\theta}$, such that $X = f_{0;\\theta} \\circ f_{1;\\theta} \\circ \\dots\n",
    "f_{D;\\theta} (Z)$. To train the network, we maximize the log sum of the\n",
    "estimated probability of sampling the sample i.e. $max \\sum \\text{log}\n",
    "p_{X;\\theta}(X_i)$. To compute this probability, we can sum over the log\n",
    "determinant of the layers, $p_{X;\\theta}(X) = \\text{log} \\left\\vert \\text{det} {\\partial z\n",
    "\\over \\partial x} \\right\\vert_{\\theta} + \\text{log} p_{Z}(Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    r\"\"\" \"\"\"\n",
    "\n",
    "    DRJIT_STRUCT = {}\n",
    "\n",
    "    def __call__(self, arg: nn.CoopVec, /) -> nn.CoopVec:\n",
    "        return (\n",
    "            0.5\n",
    "            * arg\n",
    "            * (1 + dr.tanh(dr.sqrt(2 / dr.pi) * (arg + 0.044715 * arg * arg * arg)))\n",
    "        )\n",
    "\n",
    "\n",
    "x = dr.linspace(Float32, -5, 5, 1000)\n",
    "y = GELU()(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FlowLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec: ...\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]: ...\n",
    "\n",
    "\n",
    "class PermutationLayer(FlowLayer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        z = list(z)\n",
    "        z.reverse()\n",
    "        x = nn.CoopVec(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        x = list(x)\n",
    "        x.reverse()\n",
    "        z = nn.CoopVec(x)\n",
    "        ldj = Float16(0)\n",
    "        return z, ldj\n",
    "\n",
    "\n",
    "class CouplingLayer(FlowLayer):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"net\": nn.Sequential,\n",
    "        \"config\": tuple,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, n_hidden: int = 4, width: int = 2, n_activations: int = 32\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = (width,)\n",
    "\n",
    "        sequential = []\n",
    "        sequential.append(nn.TriEncode(1, 0))\n",
    "        sequential.append(nn.Linear(-1, n_activations))\n",
    "        for i in range(n_hidden):\n",
    "            sequential.append(nn.Linear(n_activations, n_activations))\n",
    "            sequential.append(GELU())\n",
    "        sequential.append(nn.Linear(n_activations, width))\n",
    "\n",
    "        self.net = nn.Sequential(*sequential)\n",
    "\n",
    "    def inverse(self, z: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        This function represents the inverse evaluation of the coupling layer,\n",
    "        i.e. $X = f^{-1}_\\theta(Z)$.\n",
    "        \"\"\"\n",
    "        z: list = list(z)\n",
    "        d = len(z) // 2\n",
    "        id, z2 = z[:d], z[d:]\n",
    "        print(f\"{nn.CoopVec(id)=}\")\n",
    "        p = ArrayXf16(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d, :], p[d:, :]\n",
    "        x2 = (z2 - mu) * dr.exp(-a)\n",
    "        x = nn.CoopVec(id, x2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: nn.CoopVec) -> tuple[nn.CoopVec, Float16]:\n",
    "        r\"\"\"\n",
    "        This function evaluates the foward flow $Z = f_\\theta(X)$, as well as\n",
    "        the log jacobian determinant.\n",
    "        \"\"\"\n",
    "        x = list(x)\n",
    "        d = len(x) // 2\n",
    "        id, x2 = x[:d], x[d:]\n",
    "        p = ArrayXf16(self.net(nn.CoopVec(id)))\n",
    "        a, mu = p[:d, :], p[d:, :]\n",
    "        z2 = x2 * dr.exp(a) + mu\n",
    "        z = nn.CoopVec(id, z2)\n",
    "        ldj = dr.sum(a)\n",
    "        return z, ldj\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        (width,) = self.config\n",
    "        if width < 0:\n",
    "            width = size\n",
    "\n",
    "        net, _ = self.net._alloc(dtype, width // 2, rng)\n",
    "\n",
    "        result = CouplingLayer()\n",
    "        result.net = net\n",
    "\n",
    "        return result, size\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "\n",
    "    DRJIT_STRUCT = {\n",
    "        \"layers\": list[FlowLayer],\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args: FlowLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = args\n",
    "\n",
    "    def sample_base_dist(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        sample = list(sample)\n",
    "        z = []\n",
    "        for i in range(0, len(sample), 2):\n",
    "            x, y = square_to_std_normal(Array2f(sample[i], sample[i + 1]))\n",
    "            z.append(Float16(x))\n",
    "            z.append(Float16(y))\n",
    "\n",
    "        return nn.CoopVec(*z)\n",
    "\n",
    "    def eval_log_base_dist(self, z: nn.CoopVec) -> dr.ArrayBase:\n",
    "        return dr.sum([log_std_normal_pdf(z) for z in z])\n",
    "\n",
    "    def log_p(self, x: nn.CoopVec) -> Float16:\n",
    "        \"\"\"\n",
    "        This function calculates the log probability of sampling a given value\n",
    "        `x`.\n",
    "        \"\"\"\n",
    "\n",
    "        log_p = dr.zeros(x.type)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, ldj = layer.forward(x)\n",
    "            log_p += Float32(ldj)\n",
    "\n",
    "        log_p += self.eval_log_base_dist(x)\n",
    "        return log_p\n",
    "\n",
    "    def sample(self, sample: nn.CoopVec) -> nn.CoopVec:\n",
    "        r\"\"\"\n",
    "        Sample a function from the learned target distribution $X \\sim\n",
    "        p_{X;\\theta}$, given a sample from the uniform distribution.\n",
    "        \"\"\"\n",
    "        z = self.sample_base_dist(sample)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            z = layer.inverse(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _alloc(\n",
    "        self, dtype: type[dr.ArrayBase], size: int, rng: dr.random.Generator, /\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "\n",
    "        layers = []\n",
    "        for l in self.layers:\n",
    "            l_new, size = l._alloc(dtype, size, rng)\n",
    "            layers.append(l_new)\n",
    "\n",
    "        result = Flow(*layers)\n",
    "        return result, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    *[CouplingLayer(), PermutationLayer()] * 4,\n",
    "    # CouplingLayer(),\n",
    "    # PermutationLayer(),\n",
    "    # CouplingLayer(),\n",
    "]\n",
    "flow = Flow(*layers)\n",
    "\n",
    "flow: Flow = flow.alloc(TensorXf16, rng=rng)\n",
    "\n",
    "weights, flow = nn.pack(flow, \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabba3df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x = ArrayXf16(flow.sample(nn.CoopVec(rng.random(ArrayXf16, (2, 100_000)))))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.hist2d(x[0], x[1], bins=n_bins, range=hist_range)\n",
    "ax.set_title(\"flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb796a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr=0.001, params={\"weights\": Float32(weights)})\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "batch_size = 2**14\n",
    "n = 1_000\n",
    "its = []\n",
    "losses = []\n",
    "\n",
    "iterator = tqdm.tqdm(range(n))\n",
    "for it in iterator:\n",
    "    weights[:] = Float16(opt[\"weights\"])\n",
    "\n",
    "    sample1 = rng.random(Float32, batch_size)\n",
    "    sample2 = rng.random(Array2f, (2, batch_size))\n",
    "    x = ref.sample(sample1, sample2)\n",
    "    x = nn.CoopVec(ArrayXf16(x))\n",
    "\n",
    "    log_p = flow.log_p(x)\n",
    "    log_p[dr.isnan(log_p)] = 0\n",
    "    log_p[dr.isinf(log_p)] = 0\n",
    "    loss_kl = -dr.mean(log_p)\n",
    "\n",
    "    dr.backward(scaler.scale(loss_kl))\n",
    "    scaler.step(opt)\n",
    "\n",
    "    if (it + 1) % 10 == 0:\n",
    "        loss = loss_kl.numpy().item()\n",
    "        losses.append(loss)\n",
    "        its.append(it)\n",
    "        iterator.set_postfix({\"loss_kl\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(its, losses)\n",
    "plt.ylabel(\"KL loss\")\n",
    "plt.xlabel(\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ArrayXf16(flow.sample(nn.CoopVec(rng.random(ArrayXf16, (2, 100_000)))))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.hist2d(x[0], x[1], bins=n_bins, range=hist_range)\n",
    "ax.set_title(\"flow\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
